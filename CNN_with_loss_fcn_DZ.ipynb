{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xT-s6Y1FLBVk"
   },
   "outputs": [],
   "source": [
    "# data loading\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def load_dataset(base_dir):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for genre in os.listdir(base_dir):\n",
    "        genre_dir = os.path.join(base_dir, genre)\n",
    "        if not os.path.isdir(genre_dir):\n",
    "            continue\n",
    "        for fname in os.listdir(genre_dir):\n",
    "            if fname.endswith('.pt'):\n",
    "                path = os.path.join(genre_dir, fname)\n",
    "                tensor = torch.load(path).flatten().numpy()\n",
    "                features.append(tensor)\n",
    "                labels.append(genre)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "#file_dir = \"D:/Documents/A-APS360/Music_Genre_Classification/Documents/APS360/Project/train_set/processed_spectrograms_v2\"\n",
    "file_dir = \"D:/Documents/A-APS360/Music_Genre_Classification/Documents/APS360/Project/train_set_augmented/Multiple files/augmented_set\"\n",
    "\n",
    "X_train, y_train = load_dataset(file_dir + \"/train\")\n",
    "X_val, y_val = load_dataset(file_dir + \"/val\")\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_val = le.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add after loading datasets in the first cell\n",
    "# Calculate normalization parameters from training data\n",
    "train_mean = X_train.mean()\n",
    "train_std = X_train.std()\n",
    "\n",
    "# Apply normalization\n",
    "X_train = (X_train - train_mean) / train_std\n",
    "X_val = (X_val - train_mean) / train_std\n",
    "\n",
    "# Store for test data normalization\n",
    "normalization_params = {'mean': train_mean, 'std': train_std}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, SubsetRandomSampler\n",
    "\n",
    "def get_data_loader(\n",
    "    target_classes,\n",
    "    batch_size=64,\n",
    "    random_seed=1000\n",
    "):\n",
    "    \"\"\"\n",
    "    Adapted for music genre classification using pre-processed .pt files\n",
    "    Separates 35 RANDOM songs from each class in VALIDATION data for testing\n",
    "\n",
    "    Args:\n",
    "        target_classes: List of genre classes to include (or None for all)\n",
    "        batch_size: samples per batch\n",
    "        random_seed: for reproducible results\n",
    "\n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader, classes\n",
    "    \"\"\"\n",
    "\n",
    "    global X_train, X_val, y_train, y_val, le\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Convert to PyTorch tensors (labels are already encoded)\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.LongTensor(y_train)\n",
    "    X_val_tensor = torch.FloatTensor(X_val)\n",
    "    y_val_tensor = torch.LongTensor(y_val)\n",
    "\n",
    "    # Reshape to (N, 1, 128, 128)\n",
    "    X_train_tensor = X_train_tensor.view(-1, 1, 128, 128)\n",
    "    X_val_tensor = X_val_tensor.view(-1, 1, 128, 128)\n",
    "\n",
    "    # ===== Create TEST SET: 35 RANDOM songs from each class in VALIDATION data =====\n",
    "    test_indices = []\n",
    "    remaining_val_indices = []\n",
    "    \n",
    "    for class_idx in range(12):  # 12 classes\n",
    "        # Find all samples of this class in VALIDATION data\n",
    "        class_indices = np.where(y_val == class_idx)[0]\n",
    "        \n",
    "        # Shuffle the indices for this class\n",
    "        shuffled_class_indices = np.random.permutation(class_indices)\n",
    "        \n",
    "        # Take first 35 samples from shuffled indices for test (or all if less than 35)\n",
    "        n_test_samples = min(35, len(shuffled_class_indices))\n",
    "        test_class_indices = shuffled_class_indices[:n_test_samples]\n",
    "        remaining_class_indices = shuffled_class_indices[n_test_samples:]\n",
    "        \n",
    "        test_indices.extend(test_class_indices.tolist())\n",
    "        remaining_val_indices.extend(remaining_class_indices.tolist())\n",
    "        \n",
    "        print(f\"Class {le.classes_[class_idx]}: {len(class_indices)} total, {n_test_samples} for test (random), {len(remaining_class_indices)} remaining for val\")\n",
    "    \n",
    "    # Create test set from selected validation samples\n",
    "    X_test_tensor = X_val_tensor[test_indices]\n",
    "    y_test_tensor = y_val_tensor[test_indices]\n",
    "    \n",
    "    # Create new validation set with remaining samples (if any)\n",
    "    if remaining_val_indices:\n",
    "        X_val_reduced = X_val_tensor[remaining_val_indices]\n",
    "        y_val_reduced = y_val_tensor[remaining_val_indices]\n",
    "    else:\n",
    "        # If no samples remain, create empty validation set\n",
    "        X_val_reduced = torch.empty(0, 1, 128, 128)\n",
    "        y_val_reduced = torch.empty(0, dtype=torch.long)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_reduced, y_val_reduced)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) if len(val_dataset) > 0 else None\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    classes = le.classes_.tolist()\n",
    "\n",
    "    print(f\"\\nLoaded {len(train_dataset)} training, {len(val_dataset)} validation, {len(test_dataset)} test\")\n",
    "    print(f\"Test set: 35 random samples per class Ã— 12 classes = {len(test_dataset)} total\")\n",
    "    print(f\"Classes: {classes}\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "q1fMR_UTQiJo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import numpy as np\\nimport torch\\nfrom torch.utils.data import DataLoader, TensorDataset, SubsetRandomSampler\\n\\ndef get_data_loader(\\n    target_classes,\\n    batch_size=64,\\n    random_seed=1000\\n):\\n    \"\"\"\\n    Adapted for music genre classification using pre-processed .pt files\\n\\n    Args:\\n        target_classes: List of genre classes to include (or None for all)\\n        batch_size: samples per batch\\n        random_seed: for reproducible results\\n\\n    Returns:\\n        train_loader, val_loader, test_loader, classes\\n    \"\"\"\\n\\n    global X_train, X_val, y_train, y_val, le\\n\\n    torch.manual_seed(random_seed)\\n    np.random.seed(random_seed)\\n\\n    # Convert to PyTorch tensors (labels are already encoded)\\n    X_train_tensor = torch.FloatTensor(X_train)\\n    y_train_tensor = torch.LongTensor(y_train)\\n    X_val_tensor = torch.FloatTensor(X_val)\\n    y_val_tensor = torch.LongTensor(y_val)\\n\\n    # Reshape to (N, 1, 128, 128)\\n    X_train_tensor = X_train_tensor.view(-1, 1, 128, 128)\\n    X_val_tensor = X_val_tensor.view(-1, 1, 128, 128)\\n\\n    # ===== Create TEST SET from first 35 frames of each training sample =====\\n    # First 35 frames along last dimension (time)\\n    X_test_tensor = X_train_tensor[:, :, :, :35]\\n    y_test_tensor = y_train_tensor.clone()\\n\\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\\n    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\\n    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\\n\\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\\n\\n    classes = le.classes_.tolist()\\n\\n    print(f\"Loaded {len(train_dataset)} training, {len(val_dataset)} validation, {len(test_dataset)} test\")\\n    print(f\"Classes: {classes}\")\\n\\n    return train_loader, val_loader, test_loader, classes'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, SubsetRandomSampler\n",
    "\n",
    "def get_data_loader(\n",
    "    target_classes,\n",
    "    batch_size=64,\n",
    "    random_seed=1000\n",
    "):\n",
    "    \"\"\"\n",
    "    Adapted for music genre classification using pre-processed .pt files\n",
    "\n",
    "    Args:\n",
    "        target_classes: List of genre classes to include (or None for all)\n",
    "        batch_size: samples per batch\n",
    "        random_seed: for reproducible results\n",
    "\n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader, classes\n",
    "    \"\"\"\n",
    "\n",
    "    global X_train, X_val, y_train, y_val, le\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Convert to PyTorch tensors (labels are already encoded)\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.LongTensor(y_train)\n",
    "    X_val_tensor = torch.FloatTensor(X_val)\n",
    "    y_val_tensor = torch.LongTensor(y_val)\n",
    "\n",
    "    # Reshape to (N, 1, 128, 128)\n",
    "    X_train_tensor = X_train_tensor.view(-1, 1, 128, 128)\n",
    "    X_val_tensor = X_val_tensor.view(-1, 1, 128, 128)\n",
    "\n",
    "    # ===== Create TEST SET from first 35 frames of each training sample =====\n",
    "    # First 35 frames along last dimension (time)\n",
    "    X_test_tensor = X_train_tensor[:, :, :, :35]\n",
    "    y_test_tensor = y_train_tensor.clone()\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    classes = le.classes_.tolist()\n",
    "\n",
    "    print(f\"Loaded {len(train_dataset)} training, {len(val_dataset)} validation, {len(test_dataset)} test\")\n",
    "    print(f\"Classes: {classes}\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader, classes'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5at7rmNCQW7a"
   },
   "outputs": [],
   "source": [
    "def evaluate_multiclass(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_err = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            total_err += (preds != labels).sum().item()\n",
    "            total_loss += loss.item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    avg_err = total_err / total_samples\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_err, avg_loss\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_per_class(model, dataloader, le, criterion):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    report = classification_report(all_labels, all_preds, target_names=le.classes_, zero_division=0)\n",
    "    print(f\"\\nTest Set Classification Report (Avg Loss: {avg_loss:.4f}):\\n\")\n",
    "    print(report)\n",
    "\n",
    "def get_model_name(name, batch_size, learning_rate, epoch, base_dir=\"models\"):\n",
    "    # Create base directory if it doesn't exist\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "    # Format model path\n",
    "    path = os.path.join(base_dir, \"model_{0}_bs{1}_lr{2}_epoch{3}.pt\".format(\n",
    "        name, batch_size, learning_rate, epoch))\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIFpsn3jPAi6"
   },
   "source": [
    "Meefo Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Dv-lDL8dBpGT"
   },
   "outputs": [],
   "source": [
    "#stuff for model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import math\n",
    "\n",
    "# normalize data (penalizing weights so to speak)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# activation function\n",
    "import numpy as np\n",
    "from scipy.special import erf\n",
    "\n",
    "#(dont use this for now; guarantee values w/ GELU first)\n",
    "def activation(x):\n",
    "  return -x * erf(np.exp(-x))\n",
    "\n",
    "# pooling\n",
    "def pool(dim, kernel, stride = 1, padding = 0):\n",
    "  out = ((dim + 2*padding - kernel) // stride ) + 1\n",
    "  return out\n",
    "\n",
    "# residual block (prevents vanishing gradients)\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(channels)\n",
    "        )\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(x + self.conv(x))  # skip connection\n",
    "    \n",
    "# inception block (test multiple kernel sizes)\n",
    "class MiniInception(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MiniInception, self).__init__()\n",
    "        self.branch1 = nn.Conv2d(in_channels, out_channels // 4, kernel_size=1)\n",
    "        self.branch3 = nn.Conv2d(in_channels, out_channels // 4, kernel_size=3, padding=1)\n",
    "        self.branch5 = nn.Conv2d(in_channels, out_channels // 4, kernel_size=5, padding=2)\n",
    "        self.branch_pool = nn.Conv2d(in_channels, out_channels // 4, kernel_size=1)\n",
    "\n",
    "        self.activation = nn.GELU()\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b1 = self.branch1(x)\n",
    "        b3 = self.branch3(x)\n",
    "        b5 = self.branch5(x)\n",
    "        bp = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        bp = self.branch_pool(bp)\n",
    "\n",
    "        out = torch.cat([b1, b3, b5, bp], dim=1)\n",
    "        return self.activation(self.bn(out))\n",
    "\n",
    "\n",
    "# model\n",
    "class ConvBNAct(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k, s=1, p=None):\n",
    "        super().__init__()\n",
    "        if isinstance(k, int):\n",
    "            k = (k, k)\n",
    "        if p is None:\n",
    "            p = (k[0] // 2, k[1] // 2)\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=k, stride=s, padding=p, bias=False)\n",
    "        self.bn   = nn.BatchNorm2d(out_ch)\n",
    "        self.act  = nn.GELU()\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "class GlizzyNetTiny(nn.Module):\n",
    "    \"\"\"\n",
    "    3 conv blocks, large temporal kernels, optional MiniInception in block1.\n",
    "    Input: (B, 1, Freq, Time) e.g., 128,128\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int, dropout: float = 0.3, base_ch: int = 48, use_inception_first: bool = False):\n",
    "        super().__init__()\n",
    "        self.name = \"GlizzyNetTiny\"\n",
    "\n",
    "        # Block 1\n",
    "        if use_inception_first:\n",
    "            block1_feat = nn.Sequential(\n",
    "                MiniInception(1, base_ch),\n",
    "                nn.BatchNorm2d(base_ch),\n",
    "                nn.GELU()\n",
    "            )\n",
    "        else:\n",
    "            block1_feat = ConvBNAct(1, base_ch, k=(5,15))  # wider in time\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            block1_feat,\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Dropout2d(dropout * 0.5),\n",
    "        )\n",
    "\n",
    "        # Block 2\n",
    "        self.block2 = nn.Sequential(\n",
    "            ConvBNAct(base_ch, base_ch * 2, k=(3,11)),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Dropout2d(dropout * 0.75),\n",
    "        )\n",
    "\n",
    "        # Block 3\n",
    "        self.block3 = nn.Sequential(\n",
    "            ConvBNAct(base_ch * 2, base_ch * 3, k=(3,7)),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Dropout2d(dropout),\n",
    "        )\n",
    "\n",
    "        # Head\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc  = nn.Linear(base_ch * 3, num_classes)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.gap(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.fc(x)\n",
    "    \n",
    "def train_net(net, batch_size=64, learning_rate=0.01, num_epochs=30, target_classes=None, y_train=None):\n",
    "    ########################################################################\n",
    "    # Train a classifier on music genres\n",
    "    if target_classes is None:\n",
    "        raise ValueError(\"target_classes must be specified for music genre classification\")\n",
    "\n",
    "    torch.manual_seed(1000)\n",
    "\n",
    "    ########################################################################\n",
    "    # Obtain the PyTorch data loader objects to load batches of the datasets\n",
    "    global train_loader, val_loader, test_loader, classes\n",
    "    train_loader, val_loader, test_loader, classes = get_data_loader(target_classes, batch_size)\n",
    "\n",
    "    ########################################################################\n",
    "    # Define the Loss function and optimizer\n",
    "\n",
    "    # Use weighted loss\n",
    "    '''class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights_tensor = torch.FloatTensor(class_weights).to(device)'''\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    ########################################################################\n",
    "    # Set up arrays to store training/validation metrics\n",
    "    train_err = np.zeros(num_epochs)\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_err = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "\n",
    "    ########################################################################\n",
    "    # Train the network\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        total_train_loss = 0.0\n",
    "        total_train_err = 0.0\n",
    "        total_samples = 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)  # shape: [batch_size, num_classes]\n",
    "            loss = criterion(outputs, labels)  # labels are class indices [0, ..., C-1]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute number of incorrect predictions\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            total_train_err += (preds != labels).sum().item()\n",
    "            total_train_loss += loss.item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "        train_err[epoch] = total_train_err / total_samples\n",
    "        train_loss[epoch] = total_train_loss / (i + 1)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_err[epoch], val_loss[epoch] = evaluate_multiclass(net, val_loader, criterion)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: \"\n",
    "              f\"Train err: {train_err[epoch]:.4f}, Train loss: {train_loss[epoch]:.4f} | \"\n",
    "              f\"Val err: {val_err[epoch]:.4f}, Val loss: {val_loss[epoch]:.4f}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        model_path = get_model_name(net.name, batch_size, learning_rate, epoch)\n",
    "        torch.save(net.state_dict(), model_path)\n",
    "\n",
    "    print('Finished Training')\n",
    "    print(f\"Total time elapsed: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Save logs for plotting\n",
    "    np.savetxt(f\"{model_path}_train_err.csv\", train_err)\n",
    "    np.savetxt(f\"{model_path}_train_loss.csv\", train_loss)\n",
    "    np.savetxt(f\"{model_path}_val_err.csv\", val_err)\n",
    "    np.savetxt(f\"{model_path}_val_loss.csv\", val_loss)\n",
    "\n",
    "    # Save final model\n",
    "    final_model_path = get_model_name(net.name, batch_size, learning_rate, 'final')\n",
    "    torch.save(net.state_dict(), final_model_path)\n",
    "    print(f\"Final model saved to {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BcJ8iembSnbD"
   },
   "outputs": [],
   "source": [
    "# Training Curve (borrowed from lab 2)\n",
    "def plot_training_curve(path):\n",
    "    \"\"\" Plots the training curve for a model run, given the csv files\n",
    "    containing the train/validation error/loss.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    train_err = np.loadtxt(\"{}_train_err.csv\".format(path))\n",
    "    val_err = np.loadtxt(\"{}_val_err.csv\".format(path))\n",
    "    train_loss = np.loadtxt(\"{}_train_loss.csv\".format(path))\n",
    "    val_loss = np.loadtxt(\"{}_val_loss.csv\".format(path))\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Train vs Validation Error\")\n",
    "    n = len(train_err)\n",
    "    plt.plot(range(1, n+1), train_err, label=\"Train\")\n",
    "    plt.plot(range(1, n+1), val_err, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Train vs Validation Loss\")\n",
    "    plt.plot(range(1, n+1), train_loss, label=\"Train\")\n",
    "    plt.plot(range(1, n+1), val_loss, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkRglqPYMquf"
   },
   "source": [
    "Meefo Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 3060\n",
      "14452224\n",
      "23068672\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # Should be True\n",
    "print(torch.cuda.current_device())  # Should return 0\n",
    "print(torch.cuda.get_device_name(0))  # Should return something like RTX 3060\n",
    "print(torch.cuda.memory_allocated())  # Should be >0 during training\n",
    "print(torch.cuda.memory_reserved())  # Also should be >0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4Vqs75ugLo_T",
    "outputId": "ee91b4ec-0507-4cfc-ce7f-693071ecc251"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"train_net(\\n    net,\\n    batch_size=int(best_params['batch_size']),\\n    learning_rate=best_params['lr'],\\n    num_epochs=20,\\n    target_classes=target_classes,\\n    y_train=y_train\\n)\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_classes = le.classes_.tolist()\n",
    "\n",
    "\n",
    "# Run MEEFO\n",
    "'''\n",
    "best_params = run_meefo_optimization(fitness_function, search_space, pop_size=10, max_iter=20)\n",
    "\n",
    "print(\"\\nBest Hyperparameters from MEEFO:\")\n",
    "print(best_params)\n",
    "'''\n",
    "\n",
    "# Final training run with best parameters\n",
    "\n",
    "best_params = {\n",
    "    'lr': 0.000868,\n",
    "    'dropout': 0.442,\n",
    "    'hidden_size': 87,\n",
    "    'batch_size': 95\n",
    "}\n",
    "\n",
    "\n",
    "net = GlizzyNetTiny(num_classes=12, dropout=best_params['dropout'], base_ch=best_params['hidden_size'], use_inception_first=False)\n",
    "checkpoint_path = \"D:/Documents/A-APS360/Music_Genre_Classification/Documents/APS360/Project/models/model_GlizzyNetTiny_bs95_lr0.000868_epoch601.pt\"\n",
    "state_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "net.load_state_dict(state_dict)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = net.to(device)\n",
    "\n",
    "'''train_net(\n",
    "    net,\n",
    "    batch_size=int(best_params['batch_size']),\n",
    "    learning_rate=best_params['lr'],\n",
    "    num_epochs=20,\n",
    "    target_classes=target_classes,\n",
    "    y_train=y_train\n",
    ")'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnet = GlizzyNet(num_classes=12, dropout=best_params[\\'dropout\\'], use_inception_first=False)\\nnet = net.to(device)\\n\\n# Load the saved model weights\\nnet.load_state_dict(torch.load(final_model_path))\\n\\n# Continue training\\ntrain_net(\\n    net,\\n    batch_size=int(best_params[\\'batch_size\\']),\\n    learning_rate=best_params[\\'lr\\'],  # You can adjust the learning rate if needed\\n    num_epochs=300,  # Specify additional epochs #+600 (total 475+600;, so i will run for 1500) is safe\\n    target_classes=target_classes,\\n    y_train=y_train\\n)\\n\\n# Save the model again after additional training\\nnew_model_path = get_model_name(net.name, int(best_params[\\'batch_size\\']), best_params[\\'lr\\'], 574)  # Update epoch number\\ntorch.save(net.state_dict(), new_model_path)\\nprint(f\"Model saved to {new_model_path}\")'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model_path = get_model_name(net.name, int(best_params['batch_size']), best_params['lr'], 16)\n",
    "torch.save(net.state_dict(), final_model_path)\n",
    "'''\n",
    "net = GlizzyNet(num_classes=12, dropout=best_params['dropout'], use_inception_first=False)\n",
    "net = net.to(device)\n",
    "\n",
    "# Load the saved model weights\n",
    "net.load_state_dict(torch.load(final_model_path))\n",
    "\n",
    "# Continue training\n",
    "train_net(\n",
    "    net,\n",
    "    batch_size=int(best_params['batch_size']),\n",
    "    learning_rate=best_params['lr'],  # You can adjust the learning rate if needed\n",
    "    num_epochs=300,  # Specify additional epochs #+600 (total 475+600;, so i will run for 1500) is safe\n",
    "    target_classes=target_classes,\n",
    "    y_train=y_train\n",
    ")\n",
    "\n",
    "# Save the model again after additional training\n",
    "new_model_path = get_model_name(net.name, int(best_params['batch_size']), best_params['lr'], 574)  # Update epoch number\n",
    "torch.save(net.state_dict(), new_model_path)\n",
    "print(f\"Model saved to {new_model_path}\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "models\\model_GlizzyNetTiny_bs95_lr0.000868_epoch16.pt_train_err.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m final_model_path = get_model_name(net.name, \u001b[38;5;28mint\u001b[39m(best_params[\u001b[33m'\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m'\u001b[39m]), best_params[\u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m], \u001b[32m16\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mplot_training_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_model_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mplot_training_curve\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\" Plots the training curve for a model run, given the csv files\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03mcontaining the train/validation error/loss.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m train_err = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[33;43m_train_err.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m val_err = np.loadtxt(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m_val_err.csv\u001b[39m\u001b[33m\"\u001b[39m.format(path))\n\u001b[32m      9\u001b[39m train_loss = np.loadtxt(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m_train_loss.csv\u001b[39m\u001b[33m\"\u001b[39m.format(path))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\A-APS360\\Music_Genre_Classification\\Documents\\APS360\\Project\\cuda_pytorch_env\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:1397\u001b[39m, in \u001b[36mloadtxt\u001b[39m\u001b[34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[39m\n\u001b[32m   1394\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m   1395\u001b[39m     delimiter = delimiter.decode(\u001b[33m'\u001b[39m\u001b[33mlatin1\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1397\u001b[39m arr = \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m            \u001b[49m\u001b[43munpack\u001b[49m\u001b[43m=\u001b[49m\u001b[43munpack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\A-APS360\\Music_Genre_Classification\\Documents\\APS360\\Project\\cuda_pytorch_env\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:1012\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[39m\n\u001b[32m   1010\u001b[39m     fname = os.fspath(fname)\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m     fh = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_datasource\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1014\u001b[39m         encoding = \u001b[38;5;28mgetattr\u001b[39m(fh, \u001b[33m'\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlatin1\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\A-APS360\\Music_Genre_Classification\\Documents\\APS360\\Project\\cuda_pytorch_env\\Lib\\site-packages\\numpy\\lib\\_datasource.py:192\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(path, mode, destpath, encoding, newline)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[33;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[32m    157\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    188\u001b[39m \n\u001b[32m    189\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    191\u001b[39m ds = DataSource(destpath)\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\A-APS360\\Music_Genre_Classification\\Documents\\APS360\\Project\\cuda_pytorch_env\\Lib\\site-packages\\numpy\\lib\\_datasource.py:529\u001b[39m, in \u001b[36mDataSource.open\u001b[39m\u001b[34m(self, path, mode, encoding, newline)\u001b[39m\n\u001b[32m    526\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode=mode,\n\u001b[32m    527\u001b[39m                               encoding=encoding, newline=newline)\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m529\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: models\\model_GlizzyNetTiny_bs95_lr0.000868_epoch16.pt_train_err.csv not found."
     ]
    }
   ],
   "source": [
    "'''final_model_path = get_model_name(net.name, int(best_params['batch_size']), best_params['lr'], 16)\n",
    "plot_training_curve(final_model_path)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Load your BEST trained model (epoch 601)\\nbest_params = {\\n    \\'lr\\': 0.000868,\\n    \\'dropout\\': 0.442,\\n    \\'hidden_size\\': 87,\\n    \\'batch_size\\': 95\\n}\\n\\n# Create model with correct architecture\\nnet = GlizzyNetTiny(\\n    num_classes=12, \\n    dropout=best_params[\\'dropout\\'], \\n    base_ch=best_params[\\'hidden_size\\'], \\n    use_inception_first=False\\n)\\n\\n# Load your BEST checkpoint (epoch 601)\\ncheckpoint_path = \"D:/Documents/A-APS360/Music_Genre_Classification/Documents/APS360/Project/models/model_GlizzyNetTiny_bs95_lr0.000868_epoch16.pt\"\\nstate_dict = torch.load(checkpoint_path, map_location=\\'cpu\\')\\nnet.load_state_dict(state_dict)\\nnet = net.to(device)\\n\\n# Get data loaders - test_loader contains 35 samples per class FROM VALIDATION DATA\\ntarget_classes = le.classes_.tolist()\\ntrain_loader, val_loader, test_loader, classes = get_data_loader(\\n    target_classes, \\n    batch_size=95\\n)\\n\\n# Evaluate on TEST data (35 samples per class from validation set - NEVER seen during training)\\ncriterion = nn.CrossEntropyLoss()\\nprint(\"=== Performance on Test Data (35 samples per class from validation set) ===\")\\nevaluate_per_class(net, test_loader, le, criterion)\\n\\n# Optional: Also evaluate on remaining validation data (if any exists)\\nif val_loader is not None and len(val_loader.dataset) > 0:\\n    print(f\"\\n=== Performance on Remaining Validation Data ({len(val_loader.dataset)} samples) ===\")\\n    evaluate_per_class(net, val_loader, le, criterion)\\nelse:\\n    print(\"\\n=== No remaining validation data (all used for test set) ===\")'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Load your BEST trained model (epoch 601)\n",
    "best_params = {\n",
    "    'lr': 0.000868,\n",
    "    'dropout': 0.442,\n",
    "    'hidden_size': 87,\n",
    "    'batch_size': 95\n",
    "}\n",
    "\n",
    "# Create model with correct architecture\n",
    "net = GlizzyNetTiny(\n",
    "    num_classes=12, \n",
    "    dropout=best_params['dropout'], \n",
    "    base_ch=best_params['hidden_size'], \n",
    "    use_inception_first=False\n",
    ")\n",
    "\n",
    "# Load your BEST checkpoint (epoch 601)\n",
    "checkpoint_path = \"D:/Documents/A-APS360/Music_Genre_Classification/Documents/APS360/Project/models/model_GlizzyNetTiny_bs95_lr0.000868_epoch16.pt\"\n",
    "state_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "net.load_state_dict(state_dict)\n",
    "net = net.to(device)\n",
    "\n",
    "# Get data loaders - test_loader contains 35 samples per class FROM VALIDATION DATA\n",
    "target_classes = le.classes_.tolist()\n",
    "train_loader, val_loader, test_loader, classes = get_data_loader(\n",
    "    target_classes, \n",
    "    batch_size=95\n",
    ")\n",
    "\n",
    "# Evaluate on TEST data (35 samples per class from validation set - NEVER seen during training)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"=== Performance on Test Data (35 samples per class from validation set) ===\")\n",
    "evaluate_per_class(net, test_loader, le, criterion)\n",
    "\n",
    "# Optional: Also evaluate on remaining validation data (if any exists)\n",
    "if val_loader is not None and len(val_loader.dataset) > 0:\n",
    "    print(f\"\\n=== Performance on Remaining Validation Data ({len(val_loader.dataset)} samples) ===\")\n",
    "    evaluate_per_class(net, val_loader, le, criterion)\n",
    "else:\n",
    "    print(\"\\n=== No remaining validation data (all used for test set) ===\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1623479374.py, line 47)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m'''class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\u001b[39m\n       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "'''# work on test set\n",
    "def load_test_data(test_dir, le, batch_size=64):\n",
    "    \"\"\"\n",
    "    Load test data using the existing LabelEncoder\n",
    "    \n",
    "    Args:\n",
    "        test_dir: Path to directory containing test data\n",
    "        le: Existing LabelEncoder instance\n",
    "        batch_size: samples per batch\n",
    "    \n",
    "    Returns:\n",
    "        test_loader: DataLoader for test set\n",
    "    \"\"\"\n",
    "    # Load test data\n",
    "    X_test, y_test = load_dataset(test_dir)\n",
    "\n",
    "    global normalization_params\n",
    "    X_test = (X_test - normalization_params['mean']) / normalization_params['std']\n",
    "    \n",
    "    # Encode labels using existing label encoder\n",
    "    y_test_encoded = le.transform(y_test)  # Use transform() not fit_transform()\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_test_tensor = torch.LongTensor(y_test_encoded)\n",
    "    \n",
    "    # Reshape to 128x128 images\n",
    "    X_test_tensor = X_test_tensor.view(-1, 1, 128, 128)\n",
    "\n",
    "    # Create TensorDataset and DataLoader\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"Loaded {len(test_dataset)} test samples\")\n",
    "    print(f\"Classes in test set: {le.classes_.tolist()}\")\n",
    "    \n",
    "    return test_loader\n",
    "\n",
    "#plot_training_curve(final_model_path)\n",
    "\n",
    "#test_dir = \"D:\\\\Documents\\\\A-APS360\\\\Music_Genre_Classification\\\\Documents\\\\APS360\\\\Project\\\\test-set-random\\\\test set random\\\\test_updated\"\n",
    "#test_dir = \"D:/Documents/A-APS360/Music_Genre_Classification/Documents/APS360/Project/test-set-random-small/test set random/test_updated\"\n",
    "#test_dir = \"D:/Documents/A-APS360/Music_Genre_Classification/Documents/APS360/Project/test-fma/Multiple files/test_fma\"\n",
    "#test_dir = \"D:/Documents/A-APS360/Music_Genre_Classification/Documents/APS360/Project/test-fma-small/Multiple files/test_fma\"\n",
    "\n",
    "# Use weighted loss\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Load the saved model weights\n",
    "#net.load_state_dict(torch.load(final_model_path))\n",
    "\n",
    "# Move the model to the correct device\n",
    "print(f\"Using trained model: {net.name}\")\n",
    "net.eval()\n",
    "net = net.to(device)\n",
    "\n",
    "print(f\"Loaded the best model from {final_model_path}\")\n",
    "evaluate_per_class(net, test_loader, le, criterion)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6000 training, 1206 validation, 6000 test\n",
      "Classes: ['Classical', 'Electronic', 'Experimental', 'Folk', 'Hip-Hop', 'Instrumental', 'International', 'Jazz', 'Old-Time', 'Pop', 'Rock', 'Spoken']\n",
      "Selected 420 samples total (35 per class)\n",
      "=== Performance on Validation Data (Limited to 35 samples per class, 420 total) ===\n",
      "\n",
      "Test Set Classification Report (Avg Loss: 0.7520):\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Classical       0.85      0.94      0.89        35\n",
      "   Electronic       0.86      0.89      0.87        35\n",
      " Experimental       0.91      0.89      0.90        35\n",
      "         Folk       0.85      0.94      0.89        35\n",
      "      Hip-Hop       0.79      0.89      0.84        35\n",
      " Instrumental       0.93      0.71      0.81        35\n",
      "International       0.86      0.89      0.87        35\n",
      "         Jazz       0.86      0.89      0.87        35\n",
      "     Old-Time       0.82      0.66      0.73        35\n",
      "          Pop       0.75      0.86      0.80        35\n",
      "         Rock       0.94      0.89      0.91        35\n",
      "       Spoken       0.82      0.77      0.79        35\n",
      "\n",
      "     accuracy                           0.85       420\n",
      "    macro avg       0.85      0.85      0.85       420\n",
      " weighted avg       0.85      0.85      0.85       420\n",
      "\n",
      "Loaded 407 test samples\n",
      "Classes in test set: ['Classical', 'Electronic', 'Experimental', 'Folk', 'Hip-Hop', 'Instrumental', 'International', 'Jazz', 'Old-Time', 'Pop', 'Rock', 'Spoken']\n",
      "\n",
      "=== Performance on External Test Data ===\n",
      "\n",
      "Test Set Classification Report (Avg Loss: 3.1722):\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Classical       0.09      0.09      0.09        35\n",
      "   Electronic       0.13      0.09      0.10        35\n",
      " Experimental       0.18      0.14      0.16        35\n",
      "         Folk       0.17      0.09      0.11        35\n",
      "      Hip-Hop       0.05      0.06      0.06        35\n",
      " Instrumental       0.07      0.06      0.06        35\n",
      "International       0.06      0.06      0.06        35\n",
      "         Jazz       0.07      0.09      0.08        22\n",
      "     Old-Time       0.63      0.94      0.76        35\n",
      "          Pop       0.02      0.03      0.02        35\n",
      "         Rock       0.15      0.20      0.17        35\n",
      "       Spoken       0.14      0.11      0.13        35\n",
      "\n",
      "     accuracy                           0.16       407\n",
      "    macro avg       0.15      0.16      0.15       407\n",
      " weighted avg       0.15      0.16      0.15       407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''# Load your BEST trained model (epoch 601)\n",
    "best_params = {\n",
    "    'lr': 0.000868,\n",
    "    'dropout': 0.442,\n",
    "    'hidden_size': 87,\n",
    "    'batch_size': 95\n",
    "}\n",
    "\n",
    "# Create model with correct architecture\n",
    "net = GlizzyNetTiny(\n",
    "    num_classes=12, \n",
    "    dropout=best_params['dropout'], \n",
    "    base_ch=best_params['hidden_size'], \n",
    "    use_inception_first=False\n",
    ")\n",
    "\n",
    "# Load your BEST checkpoint (epoch 601) - FIX: Use epoch 601, not 16!\n",
    "checkpoint_path = \"D:/Documents/A-APS360/Music_Genre_Classification/Documents/APS360/Project/models/model_GlizzyNetTiny_bs95_lr0.000868_epoch601.pt\"\n",
    "state_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "net.load_state_dict(state_dict)\n",
    "net = net.to(device)\n",
    "\n",
    "# Get data loaders\n",
    "target_classes = le.classes_.tolist()\n",
    "train_loader, val_loader, test_loader, classes = get_data_loader(\n",
    "    target_classes, \n",
    "    batch_size=95  # Use same batch size as training\n",
    ")\n",
    "\n",
    "# Create a limited validation loader with 35 samples per class\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "\n",
    "# Get labels from validation dataset\n",
    "val_labels = np.array([val_loader.dataset[i][1] for i in range(len(val_loader.dataset))])\n",
    "\n",
    "# Find indices for first 35 samples of each class\n",
    "val_indices = []\n",
    "for class_idx in range(12):  # 12 classes\n",
    "    class_indices = np.where(val_labels == class_idx)[0]\n",
    "    # Take first 35 samples from this class (or all if less than 35)\n",
    "    selected_indices = class_indices[:35]\n",
    "    val_indices.extend(selected_indices.tolist())\n",
    "\n",
    "print(f\"Selected {len(val_indices)} samples total (35 per class)\")\n",
    "\n",
    "limited_val_dataset = Subset(val_loader.dataset, val_indices)\n",
    "limited_val_loader = DataLoader(limited_val_dataset, batch_size=95, shuffle=False)\n",
    "\n",
    "# Evaluate on LIMITED VALIDATION data (35 samples per class)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(f\"=== Performance on Validation Data (Limited to 35 samples per class, {len(limited_val_dataset)} total) ===\")\n",
    "evaluate_per_class(net, limited_val_loader, le, criterion)\n",
    "\n",
    "# Test on external test data\n",
    "test_dir = \"D:/Documents/A-APS360/Music_Genre_Classification/Documents/APS360/Project/test-fma-small/Multiple files/test_fma\"\n",
    "external_test_loader = load_test_data(test_dir, le, 95)\n",
    "\n",
    "print(\"\\n=== Performance on External Test Data ===\")\n",
    "evaluate_per_class(net, external_test_loader, le, criterion)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pygame\n",
    "import time\n",
    "\n",
    "def play_audio_pygame(file_path):\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(file_path)\n",
    "    pygame.mixer.music.set_volume(0.1) # that second clip is too loud @ 100%\n",
    "    pygame.mixer.music.play()\n",
    "    \n",
    "    # Wait for playback to finish\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    pygame.mixer.quit()\n",
    "\n",
    "# Play your 5-second MP3\n",
    "audio_file1 = \"D:/Documents/A-APS360/Music_Genre_Classification/Documents/APS360/Project/Giant Steps-5s.mp3\"\n",
    "audio_file2 = \"D:/Documents/A-APS360/Music_Genre_Classification/Documents/APS360/Project/Numb (Official Music Video) [4K UPGRADE]  Linkin Park-5s.mp3\"\n",
    "#play_audio_pygame(audio_file1)\n",
    "#play_audio_pygame(audio_file2)\n",
    "\n",
    "SR = 16000\n",
    "N_MELS = 128\n",
    "TARGET_SHAPE = (128, 128)\n",
    "MIN_AUDIO_LENGTH = 1.0 \n",
    "MIN_VOLUME = 0.01\n",
    "\n",
    "def is_valid_audio(y):\n",
    "    # check if audio contains valid signal\n",
    "    return len(y) >= SR * MIN_AUDIO_LENGTH and np.max(np.abs(y)) >= MIN_VOLUME\n",
    "\n",
    "def process_audio_file(mp3_path):\n",
    "    \"\"\"Process audio file with robust error handling\"\"\"\n",
    "    y, sr = librosa.load(mp3_path, sr=SR, mono=True)\n",
    "    if y is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        mel = librosa.feature.melspectrogram(\n",
    "            y=y, sr=sr,\n",
    "            n_mels=N_MELS,\n",
    "            n_fft=2048,\n",
    "            hop_length=512,\n",
    "            fmin=20,\n",
    "            fmax=8000\n",
    "        )\n",
    "        \n",
    "        log_mel = librosa.power_to_db(mel, ref=1.0)\n",
    "        log_mel = (log_mel - log_mel.min()) / (log_mel.max() - log_mel.min() + 1e-10) * 2 - 1\n",
    "        \n",
    "        # convert to tensor\n",
    "        tensor = torch.tensor(log_mel, dtype=torch.float32)\n",
    "        tensor = resize_spectrogram(tensor, TARGET_SHAPE)\n",
    "        return tensor.unsqueeze(0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {mp3_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def resize_spectrogram(spec, target_shape):\n",
    "    \"\"\"Pad or crop spectrogram to target shape\"\"\"\n",
    "    # Time dimension (width)\n",
    "    if spec.shape[1] < target_shape[1]:\n",
    "        spec = F.pad(spec, (0, target_shape[1] - spec.shape[1]))\n",
    "    else:\n",
    "        spec = spec[:, :target_shape[1]]\n",
    "    \n",
    "    # Frequency dimension (height)\n",
    "    if spec.shape[0] < target_shape[0]:\n",
    "        spec = F.pad(spec, (0, 0, 0, target_shape[0] - spec.shape[0]))\n",
    "    else:\n",
    "        spec = spec[:target_shape[0], :]\n",
    "    \n",
    "    return spec\n",
    "\n",
    "tensor = torch.load(\"D:/Documents/A-APS360/Music_Genre_Classification/Documents/APS360/Project/train_set/processed_spectrograms_v2/val/Jazz/5098.pt\")\n",
    "tensor2 = torch.load(\"D:/Documents/A-APS360/Music_Genre_Classification/Documents/APS360/Project/train_set/processed_spectrograms_v2/val/Rock/1100.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 12])\n",
      "Predicted genre: Jazz\n",
      "Confidence: 0.9046\n",
      "\n",
      "Top 3 predictions:\n",
      "1. Jazz: 0.9046\n",
      "2. Rock: 0.0252\n",
      "3. Experimental: 0.0237\n",
      "\n",
      "\n",
      "Output shape: torch.Size([1, 12])\n",
      "Predicted genre: Rock\n",
      "Confidence: 0.9031\n",
      "\n",
      "Top 3 predictions:\n",
      "1. Rock: 0.9031\n",
      "2. Jazz: 0.0255\n",
      "3. Pop: 0.0209\n"
     ]
    }
   ],
   "source": [
    "# initialize CNN\n",
    "best_params = {\n",
    "    'lr': 0.000868,\n",
    "    'dropout': 0.442,\n",
    "    'hidden_size': 87,\n",
    "    'batch_size': 95\n",
    "}\n",
    "\n",
    "model = GlizzyNetTiny(\n",
    "    num_classes=12, \n",
    "    dropout=best_params['dropout'], \n",
    "    base_ch=best_params['hidden_size'], \n",
    "    use_inception_first=False\n",
    ")\n",
    "\n",
    "# load the saved model weights\n",
    "checkpoint_path = \"D:/Documents/A-APS360/Music_Genre_Classification/Documents/APS360/Project/models/model_GlizzyNetTiny_bs95_lr0.000868_epoch601.pt\"\n",
    "state_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# apply normalization (same as training data)\n",
    "global normalization_params\n",
    "if 'normalization_params' in globals():\n",
    "    tensor = (tensor - normalization_params['mean']) / normalization_params['std']\n",
    "    tensor2 = (tensor2 - normalization_params['mean']) / normalization_params['std']\n",
    "\n",
    "with torch.no_grad():\n",
    "    tensor = tensor.to(device)\n",
    "    tensor2 = tensor2.to(device)\n",
    "    output1 = model(tensor.unsqueeze(0))\n",
    "    output2 = model(tensor2.unsqueeze(0))\n",
    "\n",
    "    # get first prediction\n",
    "    predicted_class_idx1 = torch.argmax(output1, dim=1).item()\n",
    "    predicted_genre1 = le.classes_[predicted_class_idx1]\n",
    "    confidence1 = torch.softmax(output1, dim=1)[0][predicted_class_idx1].item()\n",
    "\n",
    "    print(f\"Output shape: {output1.shape}\")\n",
    "    print(f\"Predicted genre: {predicted_genre1}\")\n",
    "    print(f\"Confidence: {confidence1:.4f}\")\n",
    "\n",
    "    # top 3 predictions\n",
    "    probabilities1 = torch.softmax(output1, dim=1)[0]\n",
    "    top3_indices1 = torch.topk(probabilities1, 3).indices\n",
    "    print(\"\\nTop 3 predictions:\")\n",
    "    for i, idx in enumerate(top3_indices1):\n",
    "        genre = le.classes_[idx]\n",
    "        prob = probabilities1[idx].item()\n",
    "        print(f\"{i+1}. {genre}: {prob:.4f}\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "    # get second prediction\n",
    "    predicted_class_idx2 = torch.argmax(output2, dim=1).item()\n",
    "    predicted_genre2 = le.classes_[predicted_class_idx2]\n",
    "    confidence2 = torch.softmax(output2, dim=1)[0][predicted_class_idx2].item()\n",
    "\n",
    "    print(f\"Output shape: {output2.shape}\")\n",
    "    print(f\"Predicted genre: {predicted_genre2}\")\n",
    "    print(f\"Confidence: {confidence2:.4f}\")\n",
    "\n",
    "    # top 3 predictions\n",
    "    probabilities2 = torch.softmax(output2, dim=1)[0]\n",
    "    top3_indices2 = torch.topk(probabilities2, 3).indices\n",
    "    print(\"\\nTop 3 predictions:\")\n",
    "    for i, idx in enumerate(top3_indices2):\n",
    "        genre = le.classes_[idx]\n",
    "        prob = probabilities2[idx].item()\n",
    "        print(f\"{i+1}. {genre}: {prob:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cuda_pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
